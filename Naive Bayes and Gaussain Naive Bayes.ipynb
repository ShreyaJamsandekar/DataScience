{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1aa7201",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "Make sure you have downloaded:\n",
    "- heart_processed.csv\n",
    "\n",
    "This will ask you to implement naive bayes using a custom likelihood and then comparing it against sklearn's Gaussian naive Bayes. \n",
    "The execution is slightly different from lecture and section. \n",
    "- It is more streamlined to take adavantage of vector multiplications and numpy functions, which has its own benefits if we want to scale up our naive bayes prediction to higher dimensions. \n",
    "- However, you may need to familiarize yourself with the \"dictionary\" data structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6d2714",
   "metadata": {},
   "source": [
    "## 0 Data\n",
    "Load `heart_processed.csv` from the [Heart Failure Clinical Records Dataset](https://archive.ics.uci.edu/ml/datasets/Heart%2Bfailure%2Bclinical%2Brecords)  It contains various predictors (which are in log-scale) for predicting the event of death `DEATH_EVENT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a787dd4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_8636\\2151744951.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f648bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shapes:\n",
      "\t X_train -> (209, 6)\n",
      "\t y_train -> (209,)\n",
      "test shapes:\n",
      "\t X_test -> (90, 6)\n",
      "\t y_test -> (90,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>creatinine_phosphokinase</th>\n",
       "      <th>ejection_fraction</th>\n",
       "      <th>platelets</th>\n",
       "      <th>serum_creatinine</th>\n",
       "      <th>serum_sodium</th>\n",
       "      <th>DEATH_EVENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.317488</td>\n",
       "      <td>6.366470</td>\n",
       "      <td>2.995732</td>\n",
       "      <td>12.487485</td>\n",
       "      <td>0.641854</td>\n",
       "      <td>4.867534</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.007333</td>\n",
       "      <td>8.969669</td>\n",
       "      <td>3.637586</td>\n",
       "      <td>12.481270</td>\n",
       "      <td>0.095310</td>\n",
       "      <td>4.912655</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.174387</td>\n",
       "      <td>4.983607</td>\n",
       "      <td>2.995732</td>\n",
       "      <td>11.995352</td>\n",
       "      <td>0.262364</td>\n",
       "      <td>4.859812</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.912023</td>\n",
       "      <td>4.709530</td>\n",
       "      <td>2.995732</td>\n",
       "      <td>12.254863</td>\n",
       "      <td>0.641854</td>\n",
       "      <td>4.919981</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.174387</td>\n",
       "      <td>5.075174</td>\n",
       "      <td>2.995732</td>\n",
       "      <td>12.697715</td>\n",
       "      <td>0.993252</td>\n",
       "      <td>4.753590</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>4.127134</td>\n",
       "      <td>4.110874</td>\n",
       "      <td>3.637586</td>\n",
       "      <td>11.951180</td>\n",
       "      <td>0.095310</td>\n",
       "      <td>4.962845</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>4.007333</td>\n",
       "      <td>7.506592</td>\n",
       "      <td>3.637586</td>\n",
       "      <td>12.506177</td>\n",
       "      <td>0.182322</td>\n",
       "      <td>4.934474</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>3.806662</td>\n",
       "      <td>7.630461</td>\n",
       "      <td>4.094345</td>\n",
       "      <td>13.517105</td>\n",
       "      <td>-0.223144</td>\n",
       "      <td>4.927254</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>3.806662</td>\n",
       "      <td>7.788626</td>\n",
       "      <td>3.637586</td>\n",
       "      <td>11.849398</td>\n",
       "      <td>0.336472</td>\n",
       "      <td>4.941642</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>3.912023</td>\n",
       "      <td>5.278115</td>\n",
       "      <td>3.806662</td>\n",
       "      <td>12.886641</td>\n",
       "      <td>0.470004</td>\n",
       "      <td>4.912655</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>299 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          age  creatinine_phosphokinase  ejection_fraction  platelets  \\\n",
       "0    4.317488                  6.366470           2.995732  12.487485   \n",
       "1    4.007333                  8.969669           3.637586  12.481270   \n",
       "2    4.174387                  4.983607           2.995732  11.995352   \n",
       "3    3.912023                  4.709530           2.995732  12.254863   \n",
       "4    4.174387                  5.075174           2.995732  12.697715   \n",
       "..        ...                       ...                ...        ...   \n",
       "294  4.127134                  4.110874           3.637586  11.951180   \n",
       "295  4.007333                  7.506592           3.637586  12.506177   \n",
       "296  3.806662                  7.630461           4.094345  13.517105   \n",
       "297  3.806662                  7.788626           3.637586  11.849398   \n",
       "298  3.912023                  5.278115           3.806662  12.886641   \n",
       "\n",
       "     serum_creatinine  serum_sodium  DEATH_EVENT  \n",
       "0            0.641854      4.867534            1  \n",
       "1            0.095310      4.912655            1  \n",
       "2            0.262364      4.859812            1  \n",
       "3            0.641854      4.919981            1  \n",
       "4            0.993252      4.753590            1  \n",
       "..                ...           ...          ...  \n",
       "294          0.095310      4.962845            0  \n",
       "295          0.182322      4.934474            0  \n",
       "296         -0.223144      4.927254            0  \n",
       "297          0.336472      4.941642            0  \n",
       "298          0.470004      4.912655            0  \n",
       "\n",
       "[299 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"heart_processed_log.csv\", index_col=0)\n",
    "X = dataset.drop(\"DEATH_EVENT\", axis=1).values\n",
    "y = dataset[\"DEATH_EVENT\"].values\n",
    "\n",
    "# split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# print the shapes of the training and testing sets\n",
    "print('train shapes:')\n",
    "print('\\t X_train ->', X_train.shape)\n",
    "print('\\t y_train ->', y_train.shape)\n",
    "\n",
    "print('test shapes:')\n",
    "print('\\t X_test ->', X_test.shape)\n",
    "print('\\t y_test ->', y_test.shape)\n",
    "\n",
    "display(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e9531e",
   "metadata": {},
   "source": [
    "Recall: naive Bayes is choosing the class $k$, $C_k$, that maximizes the posterior\n",
    "$$\n",
    "P(C_k \\lvert\\,\\boldsymbol{x}) = \\frac{\\pi(C_k)\\,{\\cal{}L}_{\\!\\boldsymbol{x}}(C_k)}{Z}.\n",
    "$$\n",
    "Hence, we maximize the numerator + assume that all $d$ features $x_i$ are independent (\"naive-ness\"). So we want to find the $k$ that satisfies\n",
    "$$\n",
    "\\max_k \\, \\pi(C_k)\\,{\\cal{}L}_{\\!\\boldsymbol{x}}(C_k) \\quad = \\quad \\max_k \\, \\left( \\pi(C_k)\\,\\prod_{i=1}^d p(x_i \\lvert C_k) \\right).\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca4be0c2",
   "metadata": {},
   "source": [
    "## 1 Custom Naive Bayes Classifier with KDE\n",
    "You will create a naive Bayes classifier:\n",
    "- using the training data\n",
    "- with KDE to approximate the likelihood\n",
    "- with bernoulli as the prior\n",
    "\n",
    "**Use only the training data ```X_train, y_train``` to fit the naive Bayes classifier.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b980cca8",
   "metadata": {},
   "source": [
    "### 1.1 Prior\n",
    "1. [2 pt] Compute ```prior```, a two element array. \n",
    "    - prior[0] is the probability of death event 0, $\\pi(C_0)$\n",
    "    - prior[1] is the probability of death event 1, $\\pi(C_1)$ \n",
    "    - You should construct the prior probabilities based on frequency of death events from the training data. \n",
    "    - Tip: Use np.unique() with return_counts.\n",
    "2. [1 pt] Print ```prior```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "227c44b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prior probabilities are: [0.67464115 0.32535885]\n"
     ]
    }
   ],
   "source": [
    "num = np.unique(y_train, return_counts=True)[1]\n",
    "prior = num / len(y_train) #TODO\n",
    "\n",
    "print('The prior probabilities are:', prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeefe43",
   "metadata": {},
   "source": [
    "### 1.2 Likelihood (KDE)\n",
    "1. [2 pt] Define dictionaries `kde0` and `kde1` which fulfill the following:\n",
    "    - kde0[i] corresponds to the kde object (created by calling `scipy.stats.gaussian_kde`) for feature i when death event is 0. kde1[i] defined likewise.\n",
    "    - Make sure you index the correct rows of `X_train` when defining kdes.\n",
    "    - Use bandwidth method 'scott'. (For fun, you can try 'silverman' and see what difference in result you get.)\n",
    "    - As with all arrays you throw into sklearn or scipy, you may need to take transposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2818202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: <scipy.stats._kde.gaussian_kde at 0x2a241c5fe90>,\n",
       " 1: <scipy.stats._kde.gaussian_kde at 0x2a240033590>,\n",
       " 2: <scipy.stats._kde.gaussian_kde at 0x2a210b78a50>,\n",
       " 3: <scipy.stats._kde.gaussian_kde at 0x2a241c5f3d0>,\n",
       " 4: <scipy.stats._kde.gaussian_kde at 0x2a241ec0410>,\n",
       " 5: <scipy.stats._kde.gaussian_kde at 0x2a241ec2790>}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{0: <scipy.stats._kde.gaussian_kde at 0x2a2406b8490>,\n",
       " 1: <scipy.stats._kde.gaussian_kde at 0x2a229ae5350>,\n",
       " 2: <scipy.stats._kde.gaussian_kde at 0x2a240519bd0>,\n",
       " 3: <scipy.stats._kde.gaussian_kde at 0x2a241ec0850>,\n",
       " 4: <scipy.stats._kde.gaussian_kde at 0x2a241ec2090>,\n",
       " 5: <scipy.stats._kde.gaussian_kde at 0x2a241ec2550>}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "kde0 = {} \n",
    "kde1 = {} \n",
    "\n",
    "for i in range(X_train.shape[1]):\n",
    "    X_train_death0 = X_train[y_train == 0][:, i]\n",
    "    X_train_death1 = X_train[y_train == 1][:, i]\n",
    "\n",
    "    # Compute KDE for death event 0\n",
    "    kde0[i] = gaussian_kde(X_train_death0, bw_method='scott')\n",
    "\n",
    "    # Compute KDE for death event 1\n",
    "    kde1[i] = gaussian_kde(X_train_death1, bw_method='scott')\n",
    "\n",
    "display(kde0) # Use this to check what you made. swap kde0 for kde1 if you want\n",
    "display(kde1) # Use this to check what you made. swap kde0 for kde1 if you want\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2650786a",
   "metadata": {},
   "source": [
    "2. [2 pt] Complete the code for ```compute_likelihood``` function.\n",
    "    - The objects kde0[i] and kde1[i] have a method .pdf(), which you will use when computing the likelihood.\n",
    "        - Read the documentation to understand how it works.\n",
    "    - `likelihood0[j]` is the likleihood of seeing $j$ th data $\\boldsymbol{x_j} = \\left(\\boldsymbol{x_j}_1, \\dots, \\boldsymbol{x_j}_d\\right)$ for death event 0, i.e., ${\\cal{}L}_{\\!\\boldsymbol{x_j}}(C_0) = \\prod_{i=1}^d p(\\boldsymbol{x_j}_i \\lvert C_0)$\n",
    "    - `likelihood1[j]` defined likewise.\n",
    "    - You can loop over the kde objects kde[i] to populate the likelihood arrays.\n",
    "\n",
    "(Your solution shouldn't be very complicated. A working solutions needs only about 5-10 lines of code.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6afc97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_likelihood(x, kde0, kde1):\n",
    "\n",
    "    n_samples, n_features = x.shape\n",
    "    likelihood0 = np.ones(n_samples)\n",
    "    likelihood1 = np.ones(n_samples)\n",
    "\n",
    "    for i in range(n_features):\n",
    "        likelihood0 *= kde0[i].pdf(x[:, i])\n",
    "        likelihood1 *= kde1[i].pdf(x[:, i])\n",
    "\n",
    "    likelihood = np.vstack((likelihood0, likelihood1)).T\n",
    "    return likelihood\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecf5b7a",
   "metadata": {},
   "source": [
    "### 1.3 Posterior\n",
    "1. [2 pt] Complete the code for ```compute_posterior``` function. \n",
    "    - It should include calling the function ```compute_likelihood```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdabfd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_posterior(x, prior, kde0, kde1):\n",
    "    # input:    x, a (# data) by (# features) array of test data\n",
    "    #           prior, a 1 by 2 array\n",
    "    #           kde0 and kde1, kde dictionaries that will be used to compute the likelihood\n",
    "    # output:   posterior, a (# data) by (# classes) array\n",
    "\n",
    "    likelihood = compute_likelihood(x, kde0, kde1)\n",
    "\n",
    "    # Calculate posterior probabilities\n",
    "    posterior0 = prior[0] * likelihood[:, 0]\n",
    "    posterior1 = prior[1] * likelihood[:, 1]\n",
    "\n",
    "    # Normalize to get valid probabilities\n",
    "    total_posterior = posterior0 + posterior1\n",
    "    posterior = np.vstack((posterior0 / total_posterior, posterior1 / total_posterior)).T\n",
    "\n",
    "   \n",
    "    return posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4126c3",
   "metadata": {},
   "source": [
    "### 1.4 Combine prior, likelihood, posterior\n",
    "Now, we are ready to piece all the code we prepared above.\n",
    "1. [2 pt] Complete the code for ```naive_bayes_predict```.\n",
    "    - Your code should include calling the ```compute_posterior``` function.\n",
    "    - Computing y_pred should be a simple one line of code. You may consider using numpy functions that find the index of the largest entry on every row.\n",
    "2. [1 pt] Complete the code for ```print_success_rates```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "362e2119",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from turtle import pos\n",
    "\n",
    "\n",
    "def naive_bayes_predict(x, prior, kde0, kde1):\n",
    "    # input:    x, a (# data) by (# features) array\n",
    "    #           prior, a 1 by 2 array\n",
    "    #           kde0 and kde1, kde dictionaries that will be used to compute the likelihood\n",
    "    # output:   y_pred, an array of length (# data)\n",
    "    #           y_pred[j] is the predicted class for the j-th data point in x\n",
    "    posterior = compute_posterior(x, prior, kde0, kde1)\n",
    "\n",
    "    # Choose the class with the highest posterior probability\n",
    "    y_pred = np.argmax(posterior, axis=1)\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def print_success_rates(y_true,y_pred):\n",
    "    n_success =  np.sum(y_true == y_pred)\n",
    "    n_total = len(y_true)\n",
    "\n",
    "    print(\"Number of correctly labeled points: %d of %d.  Accuracy: %.2f\"  % (n_success, n_total, n_success/n_total))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e39a565",
   "metadata": {},
   "source": [
    "### 1.5 Predict\n",
    "1. [1 pt] Use your custom naive Bayes to:\n",
    "    - predict *TRAINING* \n",
    "    - print the results with ```print_success_rates```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12b05d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correctly labeled points: 171 of 209.  Accuracy: 0.82\n"
     ]
    }
   ],
   "source": [
    "# TODO predict training data and print\n",
    "y_pred = naive_bayes_predict(X_train, prior, kde0, kde1)\n",
    "print_success_rates(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5489ddcd",
   "metadata": {},
   "source": [
    "2. [1 pt] Use your custom naive Bayes to:\n",
    "    - predict *TEST* data\n",
    "    - print the results with ```print_success_rates```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16e984d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correctly labeled points: 67 of 90.  Accuracy: 0.74\n"
     ]
    }
   ],
   "source": [
    "# TODO predict test data and print\n",
    "y_pred = naive_bayes_predict(X_test, prior, kde0, kde1)\n",
    "print_success_rates(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703d41af",
   "metadata": {},
   "source": [
    "## 2. sklearn Gaussian naive Bayes\n",
    "Let's compare our custom naive Bayes with KDE to the sklearn Gaussian naive Bayes.\n",
    "\n",
    "### 2.1 Train\n",
    "1. [1 pt] Fit ```gnb``` using training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a53e36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run sklearn's version - read up on differences if interested\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690c1058",
   "metadata": {},
   "source": [
    "### 2.2 Predict\n",
    "1. [1 pt] Use sklearn naive Bayes to:\n",
    "    - predict *TRAINING* data\n",
    "    - print the results with ```print_success_rates```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "618cb163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correctly labeled points: 160 of 209.  Accuracy: 0.77\n"
     ]
    }
   ],
   "source": [
    "# TODO predict training data and print\n",
    "gnb = GaussianNB()\n",
    "y_pred_train = gnb.fit(X_train, y_train).predict(X_train)\n",
    "print_success_rates(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c77a88",
   "metadata": {},
   "source": [
    "2. [1 pt] Use sklearn naive Bayes to:\n",
    "    - predict *TEST* data\n",
    "    - print the results with ```print_success_rates```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18d00bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correctly labeled points: 68 of 90.  Accuracy: 0.76\n"
     ]
    }
   ],
   "source": [
    "# TODO predict test data and print\n",
    "y_pred_test = gnb.fit(X_train, y_train).predict(X_test)\n",
    "print_success_rates(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a457e2dd",
   "metadata": {},
   "source": [
    "## 3. Discussion\n",
    "### 3.1 random_state = 0\n",
    "Using random_state=0 and respond to the following questions.\n",
    "\n",
    "[2 pt] For **custom NB**, what is the difference between the training and test accuracy? Give an explanation for why it might be so.\n",
    "    \n",
    "**Ans:**  There is a 10% difference in the training and testing accuracy because our custom NB required the model to understand the pattern and trends in the data. While testing on the same data it is able to correctly predict the occuracnce of death as per the features. This is due to the fact that the model is trained on that data so it can compartivly easily predict the values as compared to testing data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2338897",
   "metadata": {},
   "source": [
    "### 3.2 change random_state\n",
    "Now, experiment with a range of random_state and respond to the following question.\n",
    "\n",
    "[2 pt] Does your responses to 3.1 change? If so, describe how your responses change and why you changed them.\n",
    "- (You do not need to artificially adjust your response to 3.1 to fit the any new findings you made after changing random_state)\n",
    "\n",
    "**Ans:** If I change the random state to 2 then the testing accuracy decreases because the shuffling of the data increases. The random state is just different catogires and ways to check the reproducabilty of data. So after a certain extent (for eg when I changed the random state from 2 to 4 or 8) , the trend between the training and testing accuracy remain the same. And is I continue to increase the randome state value, same thing occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4673e8f4",
   "metadata": {},
   "source": [
    "### 3.3 Choice of model\n",
    "[2 pt]  Compare **test** accuracy results for **custom NB and sklearn GNB**? Which model would you choose to use, and why? \n",
    "\n",
    "\n",
    "**Ans:**  The tesing accuracy for my custom NB is more for this data set. And even though it decreases up to some extent, it can still learn if more data is provided. Whereas in Gaussian NB the testing accuracy is less. Hence I would choose Custom NB - as I could as more features for it to learn from as well.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8852b55f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
